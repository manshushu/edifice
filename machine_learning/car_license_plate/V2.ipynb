{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：CN\n",
      "\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：T\n",
      "\n",
      "车牌号码为：T\n",
      "\n",
      "车牌号码为：P\n",
      "\n",
      "车牌号码为：P\n",
      "\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：OO\n",
      "\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：I\n",
      "\n",
      "车牌号码为：I\n",
      "\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：14\n",
      "\n",
      "车牌号码为：14\n",
      "\n",
      "车牌号码为：\n",
      "车牌号码为：\n",
      "车牌号码为：\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m char_region \u001b[39m=\u001b[39m plate_img[\u001b[39mint\u001b[39m(h \u001b[39m*\u001b[39m \u001b[39m0.4\u001b[39m):\u001b[39mint\u001b[39m(h \u001b[39m*\u001b[39m \u001b[39m0.8\u001b[39m), \u001b[39mint\u001b[39m(w \u001b[39m*\u001b[39m \u001b[39m0.1\u001b[39m):\u001b[39mint\u001b[39m(w \u001b[39m*\u001b[39m \u001b[39m0.9\u001b[39m)]\n\u001b[0;32m     35\u001b[0m \u001b[39m# 将车牌字符区域转为灰度图和二值化图\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m gray_char_region \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(char_region, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2GRAY)\n\u001b[0;32m     37\u001b[0m th_char_region \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mthreshold(gray_char_region, \u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m, cv2\u001b[39m.\u001b[39mTHRESH_BINARY \u001b[39m+\u001b[39m cv2\u001b[39m.\u001b[39mTHRESH_OTSU)[\u001b[39m1\u001b[39m]\n\u001b[0;32m     38\u001b[0m \u001b[39m# 调用Tesseract OCR识别车牌字符\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "# 设置Tesseract OCR识别引擎路径\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# 读取图像\n",
    "img = cv2.imread('car1.jpg')\n",
    "\n",
    "# 转为灰度图\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 边缘检测\n",
    "canny_img = cv2.Canny(gray_img, 100, 200)\n",
    "\n",
    "# 寻找轮廓\n",
    "contours, hierarchy = cv2.findContours(canny_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# 筛选出符合条件的轮廓\n",
    "for contour in contours:\n",
    "    # 计算轮廓的周长\n",
    "    perimeter = cv2.arcLength(contour, True)\n",
    "\n",
    "    # 根据周长估算轮廓粗略的矩形框\n",
    "    approximate_rect = cv2.approxPolyDP(contour, 0.02 * perimeter, True)\n",
    "\n",
    "    # 筛除不符合要求的轮廓\n",
    "    if len(approximate_rect) == 4 and cv2.contourArea(contour) > 1000:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        plate_img = img[y:y + h, x:x + w]\n",
    "\n",
    "        # 在原始图像上绘制符合要求的矩形框\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "        # 计算车牌字符区域\n",
    "        char_region = plate_img[int(h * 0.4):int(h * 0.8), int(w * 0.1):int(w * 0.9)]\n",
    "\n",
    "        # 将车牌字符区域转为灰度图和二值化图\n",
    "        gray_char_region = cv2.cvtColor(char_region, cv2.COLOR_BGR2GRAY)\n",
    "        th_char_region = cv2.threshold(gray_char_region, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "        # 调用Tesseract OCR识别车牌字符\n",
    "        plate_number = pytesseract.image_to_string(th_char_region, config='--psm 10 --oem 3 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "\n",
    "        # 输出识别结果\n",
    "        print(\"车牌号码为：\" + plate_number)\n",
    "\n",
    "cv2.imshow('result', img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('mobilenet.h5')\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Pre-trained model for character recognition\n",
    "# model = tf.keras.models.load_model('D:\\code\\python\\machine_learning\\cnn_zuo_tu.h5')\n",
    "\n",
    "def segment_characters(thresholded_image):\n",
    "    \"\"\"Segment characters from the thresholded license plate image.\"\"\"\n",
    "    # Find contours in the thresholded image\n",
    "    contours, _ = cv2.findContours(thresholded_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Create a copy of the original image to draw contours on it\n",
    "    img_with_contours = np.copy(thresholded_image)\n",
    "\n",
    "    # Define a list to store character images\n",
    "    chars = []\n",
    "    \n",
    "    # Loop over all contours\n",
    "    for contour in contours:\n",
    "        # Get the bounding box of the contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Filter out contours with aspect ratio less than 0.2 or greater than 1.5\n",
    "        aspect_ratio = w / h\n",
    "        if (aspect_ratio < 0.2 or aspect_ratio > 1.5):\n",
    "            continue\n",
    "        \n",
    "        # Extract the character image from the thresholded image\n",
    "        char_image = thresholded_image[y:y+h, x:x+w]\n",
    "        \n",
    "        # Resize the character image to 28x28 pixels\n",
    "        char_image = cv2.resize(char_image, (28, 28))\n",
    "        \n",
    "        # Add a border around the character image to match the input shape of the model\n",
    "        char_image = cv2.copyMakeBorder(char_image, 2, 2, 2, 2, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "        \n",
    "        # Normalize the pixel values to be between 0 and 1\n",
    "        char_image = char_image / 255.0\n",
    "        \n",
    "        # Add the normalized character image to the list of characters\n",
    "        chars.append(char_image)\n",
    "        \n",
    "        # Draw a rectangle around the character on the original image\n",
    "        cv2.rectangle(img_with_contours, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    # Show the original image with contours drawn on it\n",
    "    cv2.imshow('Characters Segmented', img_with_contours)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "    return chars\n",
    "\n",
    "def recognize(chars):\n",
    "    \"\"\"Recognize characters using a pre-trained model.\"\"\"\n",
    "    # Loop over all characters and use the pre-trained model to predict their label\n",
    "    recognized_plate = \"\"\n",
    "    for char in chars:\n",
    "        # Reshape the character image to match the input shape of the model\n",
    "        char = np.reshape(char, (1, 32, 32, 1))\n",
    "        \n",
    "        # Use the pre-trained model to predict the label of the character\n",
    "        label = model.predict_classes(char)[0]\n",
    "        \n",
    "        # Convert the numerical label to the corresponding character\n",
    "        recognized_plate += chr(label + 65)\n",
    "    \n",
    "    return recognized_plate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load image\n",
    "img = cv2.imread('car.jpg')\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Gaussian filter\n",
    "blur = cv2.GaussianBlur(gray, (5, 5), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Canny edge detection\n",
    "edges = cv2.Canny(blur, 100, 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find contours in the edged image, keeping only the largest ones\n",
    "contours, _ = cv2.findContours(edges.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "contours = sorted(contours, key=cv2.contourArea, reverse=True)[:10]\n",
    "\n",
    "# Loop over the contours\n",
    "for contour in contours:\n",
    "    # Approximate the contour\n",
    "    peri = cv2.arcLength(contour, True)\n",
    "    approx = cv2.approxPolyDP(contour, 0.02 * peri, True)\n",
    "\n",
    "    # If our approximated contour has four points, we can assume that we have found our license plate region\n",
    "    if len(approx) == 4:\n",
    "        plate_coords = approx\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m# Perform character segmentation and recognition\u001b[39;00m\n\u001b[0;32m      8\u001b[0m characters \u001b[39m=\u001b[39m segment_characters(thresh)\n\u001b[1;32m----> 9\u001b[0m recognized_plate \u001b[39m=\u001b[39m recognize(characters)\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(recognized_plate)\n",
      "Cell \u001b[1;32mIn[3], line 65\u001b[0m, in \u001b[0;36mrecognize\u001b[1;34m(chars)\u001b[0m\n\u001b[0;32m     62\u001b[0m char \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(char, (\u001b[39m1\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m     64\u001b[0m \u001b[39m# Use the pre-trained model to predict the label of the character\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m label \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict_classes(char)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     67\u001b[0m \u001b[39m# Convert the numerical label to the corresponding character\u001b[39;00m\n\u001b[0;32m     68\u001b[0m recognized_plate \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mchr\u001b[39m(label \u001b[39m+\u001b[39m \u001b[39m65\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'predict_classes'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Extract the license plate from the image and apply thresholding to obtain a binary image\n",
    "mask = np.zeros(gray.shape, dtype=np.uint8)\n",
    "cv2.drawContours(mask, [plate_coords], -1, 255, -1)\n",
    "mask = cv2.bitwise_and(gray, gray, mask=mask)\n",
    "_, thresh = cv2.threshold(mask, 150, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Perform character segmentation and recognition\n",
    "characters = segment_characters(thresh)\n",
    "recognized_plate = recognize(characters)\n",
    "\n",
    "print(recognized_plate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf.h5\n",
      "17225924/17225924 [==============================] - 6s 0us/step\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import MobileNet\n",
    "\n",
    "model = MobileNet(weights='imagenet')\n",
    "model.save('mobilenet.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
